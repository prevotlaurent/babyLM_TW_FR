{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9d1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prevot/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "#from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ab3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRETRAINING_CORPUS_FILE = './data/fr_10M_conv/fr_10M_conv.txt'\n",
    "PRETRAINING_CORPUS_FILE = './data_raw_txt/fr_10M_wiki/wiki_fr_10M.txt'\n",
    "\n",
    "#CORPUS_FOLDER = './data/fr_10M_wiki/'\n",
    "MODEL_NAME = 'fr_10M_10K_wiki'\n",
    "#MODEL_NAME = 'fr_10M_4K_wiki'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303f548",
   "metadata": {},
   "source": [
    "# Train a tokenizer on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d88fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 50 s, sys: 3.02 s, total: 53 s\n",
      "Wall time: 3.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prevot/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#paths = [str(x) for x in Path(CORPUS_FOLDER).glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Replace('-',''),normalizers.BertNormalizer(lowercase=True)])\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=[PRETRAINING_CORPUS_FILE], vocab_size=10000, min_frequency=5, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model('./models/'+MODEL_NAME)\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./models/\"+MODEL_NAME, max_len=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4c4f7",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08301b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51206416"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=10000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe378a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prevot/anaconda3/lib/python3.11/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 616 ms, total: 1min 8s\n",
      "Wall time: 9.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create Dataset\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=PRETRAINING_CORPUS_FILE,\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "# Create Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Define other args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/\"+MODEL_NAME,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55087c9b",
   "metadata": {},
   "source": [
    "# Run Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e119b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10619' max='10619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10619/10619 40:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.718500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.531100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.137600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 25s, sys: 2.67 s, total: 40min 28s\n",
      "Wall time: 40min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "\n",
    "# Save Models\n",
    "trainer.save_model(\"./models/\"+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f5195",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1fef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./models/\"+MODEL_NAME,\n",
    "    tokenizer=\"./models/\"+MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1203bd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2275107204914093,\n",
       "  'token': 451,\n",
       "  'token_str': ' elle',\n",
       "  'sequence': 'la petite elle dort'},\n",
       " {'score': 0.14483419060707092,\n",
       "  'token': 1212,\n",
       "  'token_str': ' fille',\n",
       "  'sequence': 'la petite fille dort'},\n",
       " {'score': 0.10206688940525055,\n",
       "  'token': 897,\n",
       "  'token_str': ' petite',\n",
       "  'sequence': 'la petite petite dort'},\n",
       " {'score': 0.08099895715713501,\n",
       "  'token': 575,\n",
       "  'token_str': ' maman',\n",
       "  'sequence': 'la petite maman dort'},\n",
       " {'score': 0.019143374636769295,\n",
       "  'token': 912,\n",
       "  'token_str': ' voiture',\n",
       "  'sequence': 'la petite voiture dort'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"la petite <mask> dort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c50430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8823479413986206,\n",
       "  'token': 606,\n",
       "  'token_str': 'accord',\n",
       "  'sequence': \"ah d'accord oui\"},\n",
       " {'score': 0.03516211733222008,\n",
       "  'token': 1450,\n",
       "  'token_str': 'abord',\n",
       "  'sequence': \"ah d'abord oui\"},\n",
       " {'score': 0.023529747501015663,\n",
       "  'token': 1647,\n",
       "  'token_str': 'ailleurs',\n",
       "  'sequence': \"ah d'ailleurs oui\"},\n",
       " {'score': 0.011113138869404793,\n",
       "  'token': 1415,\n",
       "  'token_str': 'autres',\n",
       "  'sequence': \"ah d'autres oui\"},\n",
       " {'score': 0.006881808862090111,\n",
       "  'token': 832,\n",
       "  'token_str': 'autre',\n",
       "  'sequence': \"ah d'autre oui\"}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"ah d' <mask> oui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c159962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
